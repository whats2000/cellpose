"""
è³‡æ–™æª¢æŸ¥è…³æœ¬ - æª¢æŸ¥éª¨é«“å¹¹ç´°èƒå½±åƒè³‡æ–™çš„å®Œæ•´æ€§èˆ‡çµ±è¨ˆ
"""

import json
import sys
from collections import defaultdict
from pathlib import Path


def print_header(text):
    """åˆ—å°æ¨™é¡Œ"""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60)


def print_section(text):
    """åˆ—å°å€æ®µ"""
    print(f"\n{text}")
    print("-"*60)


def check_labeled_data(base_path):
    """æª¢æŸ¥å·²æ¨™è¨˜è³‡æ–™ (Roboflow)"""
    print_section("ğŸ“‹ å·²æ¨™è¨˜è³‡æ–™ (Roboflow COCO æ ¼å¼)")
    
    labeled_path = base_path / "05 è¼”ä»å¤§å­¸Roboflow"
    
    if not labeled_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {labeled_path}")
        return {}
    
    stats = {}
    
    for split in ["train", "valid", "test"]:
        split_dir = labeled_path / split
        if not split_dir.exists():
            print(f"âš ï¸  {split} è³‡æ–™å¤¾ä¸å­˜åœ¨")
            continue
        
        # çµ±è¨ˆå½±åƒ
        images = list(split_dir.glob("*.jpg")) + list(split_dir.glob("*.png"))
        
        # æª¢æŸ¥æ¨™è¨»æª”
        anno_file = split_dir / "_annotations.coco.json"
        has_annotations = anno_file.exists()
        
        n_annotations = 0
        if has_annotations:
            try:
                with open(anno_file, 'r', encoding='utf-8') as f:
                    coco_data = json.load(f)
                    n_annotations = len(coco_data.get('annotations', []))
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•è®€å–æ¨™è¨»æª”: {e}")
        
        print(f"  {split:6s}: {len(images):4d} å¼µå½±åƒ, {n_annotations:5d} å€‹æ¨™è¨»", 
              "âœ…" if has_annotations else "âŒ ç¼ºå°‘æ¨™è¨»æª”")
        
        stats[split] = {
            'n_images': len(images),
            'n_annotations': n_annotations,
            'has_annotations': has_annotations
        }
    
    total_images = sum(s['n_images'] for s in stats.values())
    total_annotations = sum(s['n_annotations'] for s in stats.values())
    
    print(f"\n  ç¸½è¨ˆ: {total_images} å¼µå½±åƒ, {total_annotations} å€‹æ¨™è¨»")
    
    return stats


def check_unlabeled_data(base_path):
    """æª¢æŸ¥æœªæ¨™è¨˜è³‡æ–™"""
    print_section("ğŸ“¦ æœªæ¨™è¨˜è³‡æ–™ (IX83 å…¨é€šé‡ç´°èƒç…§ç‰‡)")
    
    unlabeled_path = base_path / "02 IX83 å…¨é€šé‡ç´°èƒç…§ç‰‡"
    
    if not unlabeled_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {unlabeled_path}")
        return {}
    
    stats = {}
    
    for subdir_name in ["Adult MSC", "Pediatric MSC"]:
        subdir = unlabeled_path / subdir_name
        if not subdir.exists():
            print(f"âš ï¸  {subdir_name} è³‡æ–™å¤¾ä¸å­˜åœ¨")
            continue
        
        # çµ±è¨ˆå½±åƒï¼ˆéè¿´æœå°‹ï¼‰
        images = list(subdir.rglob("*.jpg")) + list(subdir.rglob("*.png")) + \
                 list(subdir.rglob("*.tif")) + list(subdir.rglob("*.tiff"))
        
        # æŒ‰æ—¥æœŸè³‡æ–™å¤¾åˆ†çµ„
        date_folders = defaultdict(int)
        for img in images:
            # å–å¾—æœ€è¿‘çš„çˆ¶è³‡æ–™å¤¾åç¨±
            parent = img.parent.name
            date_folders[parent] += 1
        
        print(f"\n  {subdir_name}:")
        print(f"    ç¸½å½±åƒæ•¸: {len(images)}")
        print(f"    æ—¥æœŸæ‰¹æ¬¡æ•¸: {len(date_folders)}")
        
        # é¡¯ç¤ºå‰å¹¾å€‹æ‰¹æ¬¡
        sorted_folders = sorted(date_folders.items(), key=lambda x: x[1], reverse=True)
        print(f"    ä¸»è¦æ‰¹æ¬¡:")
        for folder, count in sorted_folders[:5]:
            print(f"      - {folder}: {count} å¼µ")
        
        if len(sorted_folders) > 5:
            print(f"      ... é‚„æœ‰ {len(sorted_folders) - 5} å€‹æ‰¹æ¬¡")
        
        stats[subdir_name] = {
            'n_images': len(images),
            'n_batches': len(date_folders),
            'batches': dict(sorted_folders)
        }
    
    total_unlabeled = sum(s['n_images'] for s in stats.values())
    print(f"\n  ç¸½è¨ˆ: {total_unlabeled} å¼µæœªæ¨™è¨˜å½±åƒ")
    
    return stats


def check_workspace():
    """æª¢æŸ¥å·¥ä½œç›®éŒ„"""
    print_section("ğŸ”§ å·¥ä½œç›®éŒ„ç‹€æ…‹")
    
    workspace = Path("active_learning_workspace")
    
    if not workspace.exists():
        print(f"  â„¹ï¸  å·¥ä½œç›®éŒ„å°šæœªå»ºç«‹: {workspace}")
        print(f"     åŸ·è¡Œä¸»å‹•å­¸ç¿’ç³»çµ±æ™‚æœƒè‡ªå‹•å»ºç«‹")
        return
    
    # æª¢æŸ¥å­ç›®éŒ„
    subdirs = ["models", "annotations", "predictions", "logs"]
    for subdir in subdirs:
        subdir_path = workspace / subdir
        exists = subdir_path.exists()
        status = "âœ…" if exists else "âŒ"
        print(f"  {status} {subdir}/")
    
    # æª¢æŸ¥è¿­ä»£ç›®éŒ„
    iterations = sorted(workspace.glob("iteration_*"))
    if iterations:
        print(f"\n  å·²å®Œæˆè¿­ä»£: {len(iterations)}")
        for iter_dir in iterations[-3:]:  # é¡¯ç¤ºæœ€è¿‘3å€‹
            print(f"    - {iter_dir.name}")


def check_environment():
    """æª¢æŸ¥ Python ç’°å¢ƒ"""
    print_section("ğŸ Python ç’°å¢ƒæª¢æŸ¥")
    
    # Python ç‰ˆæœ¬
    python_version = sys.version.split()[0]
    print(f"  Python ç‰ˆæœ¬: {python_version}")
    
    # æª¢æŸ¥å¥—ä»¶
    required_packages = [
        "cellpose",
        "torch",
        "numpy",
        "cv2",
        "sklearn",
        "pandas",
        "matplotlib"
    ]
    
    print("\n  å¿…è¦å¥—ä»¶:")
    for package in required_packages:
        try:
            if package == "cv2":
                import cv2
                version = getattr(cv2, '__version__', 'installed')
            elif package == "sklearn":
                import sklearn
                version = sklearn.__version__
            else:
                mod = __import__(package)
                version = getattr(mod, '__version__', 'unknown')
            
            print(f"    âœ… {package:12s} (v{version})")
        except ImportError:
            print(f"    âŒ {package:12s} (æœªå®‰è£)")
    
    # GPU æª¢æŸ¥
    print("\n  GPU ç‹€æ…‹:")
    try:
        import torch
        if torch.cuda.is_available():
            print(f"    âœ… CUDA å¯ç”¨")
            print(f"       è£ç½®: {torch.cuda.get_device_name(0)}")
            print(f"       è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print(f"    âš ï¸  CUDA ä¸å¯ç”¨ (å°‡ä½¿ç”¨ CPUï¼Œè¨“ç·´æœƒè¼ƒæ…¢)")
    except Exception as e:
        print(f"    âŒ ç„¡æ³•æª¢æŸ¥ GPU: {e}")


def estimate_resources():
    """ä¼°ç®—è³‡æºéœ€æ±‚"""
    print_section("ğŸ“Š è³‡æºéœ€æ±‚ä¼°ç®—")
    
    print("  åŸºæ–¼ç•¶å‰è³‡æ–™è¦æ¨¡çš„ä¼°ç®—:")
    print()
    print("  è¨“ç·´è³‡æº:")
    print("    - GPU è¨˜æ†¶é«”: å»ºè­° >= 8GB")
    print("    - RAM: å»ºè­° >= 32GB")
    print("    - å„²å­˜ç©ºé–“: å»ºè­° >= 500GB")
    print()
    print("  æ™‚é–“ä¼°ç®— (10 è¼ªè¿­ä»£):")
    print("    - æ¯è¼ªæ¨™è¨»: ~10-15 å°æ™‚ (100 å¼µå½±åƒ)")
    print("    - æ¯è¼ªè¨“ç·´: ~2-4 å°æ™‚ (è¦– GPU è€Œå®š)")
    print("    - ç¸½è¨ˆç´„: 120-190 å°æ™‚ (ç´„ 3-4 å€‹æœˆ)")
    print()
    print("  äººåŠ›éœ€æ±‚:")
    print("    - AI å·¥ç¨‹å¸«: 1-2 äºº")
    print("    - ç”Ÿç‰©é†«å­¸å°ˆå®¶: 1-2 äºº (å“è³ªæ§åˆ¶)")
    print("    - æ¨™è¨»äººå“¡: 2-3 äºº")


def generate_summary_report(labeled_stats, unlabeled_stats):
    """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
    print_header("ğŸ“ˆ è³‡æ–™æ‘˜è¦å ±å‘Š")
    
    total_labeled = sum(s['n_images'] for s in labeled_stats.values())
    total_unlabeled = sum(s['n_images'] for s in unlabeled_stats.values())
    
    print(f"""
  å·²æ¨™è¨˜è³‡æ–™: {total_labeled:5d} å¼µå½±åƒ
  æœªæ¨™è¨˜è³‡æ–™: {total_unlabeled:5d} å¼µå½±åƒ
  
  æ¨™è¨˜/æœªæ¨™è¨˜æ¯”ä¾‹: 1:{total_unlabeled/max(total_labeled, 1):.0f}
  
  å»ºè­°:
  """)
    
    if total_labeled < 50:
        print("  âš ï¸  å·²æ¨™è¨˜è³‡æ–™è¼ƒå°‘ (<50)ï¼Œå»ºè­°:")
        print("     1. å…ˆæ‰‹å‹•æ¨™è¨»æ›´å¤šè³‡æ–™ (è‡³å°‘ 50-100 å¼µ)")
        print("     2. æˆ–ç›´æ¥é–‹å§‹ä¸»å‹•å­¸ç¿’ï¼Œä½†åˆå§‹æ•ˆèƒ½å¯èƒ½è¼ƒä½")
    elif total_labeled < 200:
        print("  âœ… å·²æ¨™è¨˜è³‡æ–™é©ä¸­ï¼Œå¯ä»¥é–‹å§‹ä¸»å‹•å­¸ç¿’")
    else:
        print("  âœ… å·²æ¨™è¨˜è³‡æ–™å……è¶³ï¼Œé©åˆè¨“ç·´")
    
    if total_unlabeled > 10000:
        print(f"\n  â„¹ï¸  æœªæ¨™è¨˜è³‡æ–™é‡é¾å¤§ ({total_unlabeled} å¼µ)")
        print("     å»ºè­°å…ˆå°è³‡æ–™é€²è¡Œæ¡æ¨£æˆ–åˆ†æ‰¹è™•ç†")


def save_stats_to_file(labeled_stats, unlabeled_stats):
    """å„²å­˜çµ±è¨ˆè³‡æ–™åˆ°æª”æ¡ˆ"""
    stats = {
        'labeled': labeled_stats,
        'unlabeled': unlabeled_stats,
        'timestamp': str(Path('.').__str__())
    }
    
    output_file = Path("data_statistics.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ çµ±è¨ˆè³‡æ–™å·²å„²å­˜è‡³: {output_file}")


def main():
    """ä¸»ç¨‹å¼"""
    print_header("éª¨é«“å¹¹ç´°èƒå½±åƒè³‡æ–™æª¢æŸ¥å·¥å…·")
    
    # è¨­å®šåŸºç¤è·¯å¾‘
    base_path = Path("raw_data/01 AI åˆ†æç´°èƒç…§ç‰‡")
    
    if not base_path.exists():
        print(f"\nâŒ æ‰¾ä¸åˆ°è³‡æ–™æ ¹ç›®éŒ„: {base_path}")
        print(f"   è«‹ç¢ºèªæ‚¨åœ¨æ­£ç¢ºçš„å°ˆæ¡ˆç›®éŒ„ä¸‹åŸ·è¡Œæ­¤è…³æœ¬")
        return
    
    # åŸ·è¡Œå„é …æª¢æŸ¥
    labeled_stats = check_labeled_data(base_path)
    unlabeled_stats = check_unlabeled_data(base_path)
    check_workspace()
    check_environment()
    estimate_resources()
    generate_summary_report(labeled_stats, unlabeled_stats)
    
    # å„²å­˜çµ±è¨ˆ
    try:
        save_stats_to_file(labeled_stats, unlabeled_stats)
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•å„²å­˜çµ±è¨ˆè³‡æ–™: {e}")
    
    print("\n" + "="*60)
    print("  æª¢æŸ¥å®Œæˆï¼")
    print("="*60 + "\n")
    
    # ä¸‹ä¸€æ­¥å»ºè­°
    print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:")
    print("  1. å¦‚æœç’°å¢ƒæª¢æŸ¥æœ‰ç¼ºå°‘çš„å¥—ä»¶ï¼Œè«‹å…ˆå®‰è£")
    print("  2. åŸ·è¡Œ scripts/tile_large_images.py è™•ç†å¤§å‹å½±åƒ")
    print("  3. åŸ·è¡Œ scripts/convert_coco_to_cellpose.py è½‰æ›æ ¼å¼")
    print("  4. åŸ·è¡Œ src/active_learning_framework.py é–‹å§‹è¨“ç·´\n")


if __name__ == "__main__":
    main()
